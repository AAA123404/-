{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAA123404/Financial-big-data-analysis/blob/main/week11%E4%BD%9C%E6%A5%AD_%E5%A4%9A%E6%A8%A1%E6%85%8B_Multimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uSCX3Etabbo",
        "outputId": "ba7eea7f-a615-4071-d6f7-6a007c820f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flC6CwXHrKL-",
        "outputId": "35947f91-a469-41b9-e0bd-0d52a2978365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "arY_Uk3DJnYy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import Series,DataFrame\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import date\n",
        "import re\n",
        "import gensim\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import pickle as pk\n",
        "from gensim.models import Word2Vec\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xnFp9-bwJo_G"
      },
      "outputs": [],
      "source": [
        "def data2change(data):\n",
        "    change = pd.DataFrame(data).pct_change()\n",
        "    change = change.replace([np.inf, -np.inf], np.nan)\n",
        "    change = change.fillna(0.).values.tolist()\n",
        "    change = [c[0] for c in change]\n",
        "    return change"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Qsi8a0b1jHOd"
      },
      "outputs": [],
      "source": [
        "def load_text_csv(filename = '/content/drive/MyDrive/金融/week11/Combined_News_DJIA.csv', date_split = date(2014,12,31)):\n",
        "  df = pd.read_csv(filename)\n",
        "\n",
        "  df['Combined']=df.iloc[:,2:27].apply(lambda row: ''.join(str(row.values)), axis=1)\n",
        "  date_split = pd.to_datetime(date_split)\n",
        "  train = df.loc[(pd.to_datetime(df[\"Date\"]) <= date_split),['Label','Combined']]\n",
        "  test = df.loc[(pd.to_datetime(df[\"Date\"]) > date_split),['Label','Combined']]\n",
        "\n",
        "  return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gdjGy7WHjTlB"
      },
      "outputs": [],
      "source": [
        "train, test = load_text_csv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ycq_TK2lJqlU"
      },
      "outputs": [],
      "source": [
        "def load_ts_csv(filename = '/content/drive/MyDrive/金融/week11/DJIA_table.csv', date_split = date(2014,12,31)):\n",
        "  data_original = pd.read_csv(filename)[::-1]\n",
        "  data_original[\"Date\"] = pd.to_datetime(data_original[\"Date\"], format='%d-%m-%Y')\n",
        "  date_split = pd.to_datetime(date_split)\n",
        "\n",
        "  train2 = data_original.loc[(pd.to_datetime(data_original[\"Date\"]) <= date_split)]\n",
        "  test2 = data_original.loc[(pd.to_datetime(data_original[\"Date\"]) > date_split)]\n",
        "\n",
        "  open_train = train2.loc[:, 'Open']\n",
        "  open_test = test2.loc[:, 'Open']\n",
        "\n",
        "  high_train = train2.loc[:, 'High']\n",
        "  high_test = test2.loc[:, 'High']\n",
        "\n",
        "  low_train = train2.loc[:, 'Low']\n",
        "  low_test = test2.loc[:, 'Low']\n",
        "\n",
        "  close_train = train2.loc[:, 'Close']\n",
        "  close_test = test2.loc[:, 'Close']\n",
        "\n",
        "  volume_train = train2.loc[:, 'Volume']\n",
        "  volume_test = test2.loc[:, 'Volume']\n",
        "\n",
        "  open_train = data2change(open_train)\n",
        "  open_test = data2change(open_test)\n",
        "\n",
        "  high_train = data2change(high_train)\n",
        "  high_test = data2change(high_test)\n",
        "\n",
        "  low_train = data2change(low_train)\n",
        "  low_test = data2change(low_test)\n",
        "\n",
        "  close_test = data2change(close_test)\n",
        "  close_train = data2change(close_train)\n",
        "\n",
        "  volume_train = data2change(volume_train)\n",
        "  volume_test = data2change(volume_test)\n",
        "\n",
        "  train = np.column_stack((open_train, high_train, low_train, close_train, volume_train))\n",
        "  test = np.column_stack((open_test, high_test, low_test, close_test, volume_test))\n",
        "\n",
        "  print(train.shape)\n",
        "  print(test.shape)\n",
        "\n",
        "  return train, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPe_71FKlpFM",
        "outputId": "e597b43d-4fec-437b-d153-beef39dc5a0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1611, 5)\n",
            "(252, 5)\n"
          ]
        }
      ],
      "source": [
        "data_chng_train, data_chng_test = load_ts_csv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0gzH1M0GJwBz"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def text_process(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    # tokenizing\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    text_processed = tokenizer.tokenize(text)\n",
        "\n",
        "    # 去除停用詞\n",
        "    text_processed = [word.lower() for word in text_processed if word.lower() not in stop_words]\n",
        "\n",
        "    # 詞幹處理\n",
        "    porter_stemmer = PorterStemmer()\n",
        "    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
        "\n",
        "    # 移除 'b' 這樣的詞\n",
        "    text_processed = [word for word in text_processed if word != 'b']\n",
        "\n",
        "    return \" \".join(text_processed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GlVlL_fBJ0_I"
      },
      "outputs": [],
      "source": [
        "def transform_text2sentences(train, test, save_train = '/content/drive/MyDrive/金融/week11/train_text.p', save_test = '/content/drive/MyDrive/金融/week11/test_text.p'):\n",
        "\n",
        "\ttrain_text = []\n",
        "\ttest_text = []\n",
        "\tfor each in train['Combined']:\n",
        "\t    train_text.append(text_process(each))\n",
        "\tfor each in test['Combined']:\n",
        "\t    test_text.append(text_process(each))\n",
        "\n",
        "\tif save_train != None: pk.dump(train_text, open(save_train, 'wb'))\n",
        "\tif save_test != None: pk.dump(test_text, open(save_test, 'wb'))\n",
        "\n",
        "\treturn train_text, test_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiJjlPV2oCHJ"
      },
      "outputs": [],
      "source": [
        "transform_text2sentences(train,test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DGa5u8TNJ4ip"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def transform_text_into_vectors(train_text, test_text, embedding_size=100, model_path='word2vec10.model'):\n",
        "    data_for_w2v = []\n",
        "    for text in train_text + test_text:\n",
        "        words = text.split(' ')\n",
        "        data_for_w2v.append(words)\n",
        "\n",
        "    # 訓練 Word2Vec 模型\n",
        "    model = Word2Vec(data_for_w2v, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
        "    model.save(model_path)\n",
        "\n",
        "    # 計算文本的向量\n",
        "    def get_sentence_vector(sentence, model):\n",
        "        # 將每個單詞轉換為向量，並處理不在詞彙表中的單詞\n",
        "        sentence_vectors = []\n",
        "        for word in sentence.split(' '):\n",
        "            if word in model.wv:\n",
        "                sentence_vectors.append(model.wv[word])\n",
        "        if sentence_vectors:\n",
        "            return np.mean(sentence_vectors, axis=0)\n",
        "        else:\n",
        "            return np.zeros(model.vector_size)\n",
        "\n",
        "    train_text_vectors = [get_sentence_vector(sentence, model) for sentence in train_text]\n",
        "    test_text_vectors = [get_sentence_vector(sentence, model) for sentence in test_text]\n",
        "\n",
        "    return train_text_vectors, test_text_vectors, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TKIMd6-Ir9dQ"
      },
      "outputs": [],
      "source": [
        "train_text = pk.load(open('/content/drive/MyDrive/金融/week11/train_text.p', 'rb'))[1:]\n",
        "test_text = pk.load(open('/content/drive/MyDrive/金融/week11/test_text.p', 'rb'))[1:]\n",
        "\n",
        "train_text_vectors, test_text_vectors, model = transform_text_into_vectors(train_text, test_text, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C0qy73Lkd6nm"
      },
      "outputs": [],
      "source": [
        "def split_into_XY(data_chng_train, train_text_vectors, step, window, forecast):\n",
        "    X_train, X_train_text, Y_train, Y_train2 = [], [], [], []\n",
        "    for i in range(0, len(data_chng_train), step):\n",
        "        try:\n",
        "            if i + window + forecast >= len(data_chng_train):\n",
        "                continue\n",
        "\n",
        "            x_i = data_chng_train[i:i+window]\n",
        "            y_i = np.std(data_chng_train[i:i+window+forecast][3]) #close\n",
        "\n",
        "            text_average = train_text_vectors[i:i+window]\n",
        "            last_close = x_i[-1]\n",
        "\n",
        "            y_i2 = None\n",
        "            if data_chng_train[i+window+forecast][3] > 0.:\n",
        "                y_i2 = 1.\n",
        "            else:\n",
        "                y_i2 = 0.\n",
        "\n",
        "        except IndexError as e:\n",
        "            print('IndexError:', e)\n",
        "            continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Error:', e)\n",
        "            continue\n",
        "\n",
        "        X_train.append(x_i)\n",
        "        X_train_text.append(text_average)\n",
        "        Y_train.append(y_i)\n",
        "        Y_train2.append(y_i2)\n",
        "\n",
        "    X_train, X_train_text, Y_train, Y_train2 = np.array(X_train), np.array(X_train_text), np.array(Y_train), np.array(Y_train2)\n",
        "    return X_train, X_train_text, Y_train, Y_train2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kYvKuAx8sBvY"
      },
      "outputs": [],
      "source": [
        "X_train, X_train_text, Y_train, Y_train2 = split_into_XY(data_chng_train, train_text_vectors, 1, 30, 1)\n",
        "X_test, X_test_text, Y_test, Y_test2 = split_into_XY(data_chng_test, test_text_vectors, 1, 30, 1)\n",
        "\n",
        "\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 5))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzpELsWVDhgL",
        "outputId": "717a499c-62f7-412f-e6f6-bbb5b1ef707c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1580, 30, 5)\n",
            "(1580, 30, 100)\n",
            "(1580,)\n",
            "(1580,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_train_text.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_train2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZG5AOwmIli0",
        "outputId": "41d230c6-4e26-4429-af0a-29413163111b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(221, 30, 5)\n",
            "(221, 30, 100)\n",
            "(221,)\n",
            "(221,)\n"
          ]
        }
      ],
      "source": [
        "print(X_test.shape)\n",
        "print(X_test_text.shape)\n",
        "print(Y_test.shape)\n",
        "print(Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vP2_uSGIvZm",
        "outputId": "49f4b9b6-0a54-42db-8a35-1335befc3cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.02542046 0.05076908 0.13608661 ... 0.0138336  0.0963691  0.06242205]\n"
          ]
        }
      ],
      "source": [
        "print(Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSZ9iigZ8gWB"
      },
      "source": [
        "#訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "WLsVX9hgtbR6",
        "outputId": "a5fba25b-aa0f-4da9-9f72-f880931e6b99"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ ts_input (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m5\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ text_input (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m10\u001b[0m)         │            \u001b[38;5;34m640\u001b[0m │ ts_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m10\u001b[0m)         │          \u001b[38;5;34m4,440\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m840\u001b[0m │ lstm_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │            \u001b[38;5;34m840\u001b[0m │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ lstm_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ lstm_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ flatten_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m1,344\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m1,344\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_3 (\u001b[38;5;33mLeakyReLU\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_2 (\u001b[38;5;33mLeakyReLU\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ leaky_re_lu_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ regression (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m65\u001b[0m │ leaky_re_lu_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ class (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m65\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ ts_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ text_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ ts_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,440</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">840</span> │ lstm_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">840</span> │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ flatten_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ leaky_re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ leaky_re_lu_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ regression (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ leaky_re_lu_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ class (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,578\u001b[0m (37.41 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,578</span> (37.41 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,578\u001b[0m (37.41 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,578</span> (37.41 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, LeakyReLU, Dropout, Flatten, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "main_input = Input(shape=(30, 5), name='ts_input')\n",
        "text_input = Input(shape=(30, 100), name='text_input')\n",
        "\n",
        "lstm1 = LSTM(10, return_sequences=True, recurrent_dropout=0.25, dropout=0.25, bias_initializer='ones')(main_input)\n",
        "lstm1 = LSTM(10, return_sequences=False, recurrent_dropout=0.25, dropout=0.25, bias_initializer='ones')(lstm1)\n",
        "lstm1 = Flatten()(lstm1)\n",
        "\n",
        "lstm2 = LSTM(10, return_sequences=True, recurrent_dropout=0.25, dropout=0.25, bias_initializer='ones')(text_input)\n",
        "lstm2 = LSTM(10, return_sequences=False, recurrent_dropout=0.25, dropout=0.25, bias_initializer='ones')(lstm2)\n",
        "lstm2 = Flatten()(lstm2)\n",
        "\n",
        "lstms = concatenate([lstm1, lstm2])\n",
        "\n",
        "x1 = Dense(64)(lstms)\n",
        "x1 = LeakyReLU()(x1)\n",
        "x1 = Dense(1, activation='linear', name='regression')(x1)\n",
        "\n",
        "x2 = Dense(64)(lstms)\n",
        "x2 = LeakyReLU()(x2)\n",
        "x2 = Dropout(0.9)(x2)\n",
        "x2 = Dense(1, activation='sigmoid', name='class')(x2)\n",
        "\n",
        "final_model = Model(inputs=[main_input, text_input], outputs=[x1, x2])\n",
        "\n",
        "\n",
        "final_model.compile(optimizer=Adam(), loss={'regression': 'mse', 'class': 'binary_crossentropy'}, metrics={'regression': 'mae', 'class': 'accuracy'})\n",
        "\n",
        "final_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4lIg8_D2tulP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpointer = ModelCheckpoint(monitor='val_loss',\n",
        "                               filepath=\"/content/drive/MyDrive/金融/week11/model.keras\",\n",
        "                               verbose=1,\n",
        "                               save_best_only=True)\n",
        "\n",
        "opt = Nadam(learning_rate=0.0002, clipnorm=0.5)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=50, min_lr=0.000001, verbose=1)\n",
        "\n",
        "final_model.compile(optimizer=opt,\n",
        "                    loss={'regression': 'mse', 'class': 'binary_crossentropy'},\n",
        "                    loss_weights={'regression': 1., 'class': 0.2})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkN9khteuOoY",
        "outputId": "a047588a-b1c9-4047-f2b4-5ca8e11ac3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - class_loss: 0.3353 - loss: 1.9588 - regression_loss: 1.6114\n",
            "Epoch 1: val_loss improved from inf to 0.69134, saving model to /content/drive/MyDrive/金融/week11/model.keras\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 336ms/step - class_loss: 0.3344 - loss: 1.9240 - regression_loss: 1.5683 - val_class_loss: 0.2508 - val_loss: 0.6913 - val_regression_loss: 0.4406 - learning_rate: 2.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - class_loss: 0.3268 - loss: 0.8622 - regression_loss: 0.5306\n",
            "Epoch 2: val_loss improved from 0.69134 to 0.52726, saving model to /content/drive/MyDrive/金融/week11/model.keras\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - class_loss: 0.3251 - loss: 0.8526 - regression_loss: 0.5192 - val_class_loss: 0.2500 - val_loss: 0.5273 - val_regression_loss: 0.2772 - learning_rate: 2.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - class_loss: 0.3198 - loss: 0.5826 - regression_loss: 0.2628\n",
            "Epoch 3: val_loss improved from 0.52726 to 0.43741, saving model to /content/drive/MyDrive/金融/week11/model.keras\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - class_loss: 0.3165 - loss: 0.5755 - regression_loss: 0.2559 - val_class_loss: 0.2496 - val_loss: 0.4374 - val_regression_loss: 0.1878 - learning_rate: 2.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - class_loss: 0.3130 - loss: 0.4840 - regression_loss: 0.1710\n",
            "Epoch 4: val_loss improved from 0.43741 to 0.37954, saving model to /content/drive/MyDrive/金融/week11/model.keras\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - class_loss: 0.3134 - loss: 0.4817 - regression_loss: 0.1675 - val_class_loss: 0.2494 - val_loss: 0.3795 - val_regression_loss: 0.1302 - learning_rate: 2.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - class_loss: 0.3118 - loss: 0.4335 - regression_loss: 0.1217\n",
            "Epoch 5: val_loss improved from 0.37954 to 0.34023, saving model to /content/drive/MyDrive/金融/week11/model.keras\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - class_loss: 0.3146 - loss: 0.4326 - regression_loss: 0.1190 - val_class_loss: 0.2494 - val_loss: 0.3402 - val_regression_loss: 0.0908 - learning_rate: 2.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - class_loss: 0.3040 - loss: 0.3920 - regression_loss: 0.0874\n",
            "Epoch 6: val_loss improved from 0.34023 to 0.31545, saving model to /content/drive/MyDrive/金融/week11/model.keras\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - class_loss: 0.3042 - loss: 0.3922 - regression_loss: 0.0869 - val_class_loss: 0.2497 - val_loss: 0.3155 - val_regression_loss: 0.0657 - learning_rate: 2.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - class_loss: 0.3145 - loss: 0.3891 - regression_loss: 0.0748\n",
            "Epoch 7: val_loss improved from 0.31545 to 0.30801, saving model to /content/drive/MyDrive/金融/week11/model.keras\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - class_loss: 0.3142 - loss: 0.3888 - regression_loss: 0.0750 - val_class_loss: 0.2501 - val_loss: 0.3080 - val_regression_loss: 0.0579 - learning_rate: 2.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - class_loss: 0.3046 - loss: 0.3789 - regression_loss: 0.0746\n",
            "Epoch 8: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - class_loss: 0.3044 - loss: 0.3784 - regression_loss: 0.0745 - val_class_loss: 0.2506 - val_loss: 0.3089 - val_regression_loss: 0.0583 - learning_rate: 2.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - class_loss: 0.3009 - loss: 0.3784 - regression_loss: 0.0767\n",
            "Epoch 9: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - class_loss: 0.3004 - loss: 0.3777 - regression_loss: 0.0759 - val_class_loss: 0.2513 - val_loss: 0.3112 - val_regression_loss: 0.0599 - learning_rate: 2.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - class_loss: 0.3024 - loss: 0.3742 - regression_loss: 0.0713\n",
            "Epoch 10: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - class_loss: 0.3018 - loss: 0.3740 - regression_loss: 0.0713 - val_class_loss: 0.2519 - val_loss: 0.3136 - val_regression_loss: 0.0617 - learning_rate: 2.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - class_loss: 0.3012 - loss: 0.3711 - regression_loss: 0.0700\n",
            "Epoch 11: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - class_loss: 0.3020 - loss: 0.3714 - regression_loss: 0.0698 - val_class_loss: 0.2523 - val_loss: 0.3129 - val_regression_loss: 0.0606 - learning_rate: 2.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - class_loss: 0.2905 - loss: 0.3601 - regression_loss: 0.0695\n",
            "Epoch 12: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - class_loss: 0.2906 - loss: 0.3604 - regression_loss: 0.0696 - val_class_loss: 0.2524 - val_loss: 0.3120 - val_regression_loss: 0.0595 - learning_rate: 2.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - class_loss: 0.2806 - loss: 0.3521 - regression_loss: 0.0714\n",
            "Epoch 13: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - class_loss: 0.2825 - loss: 0.3534 - regression_loss: 0.0711 - val_class_loss: 0.2527 - val_loss: 0.3118 - val_regression_loss: 0.0591 - learning_rate: 2.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - class_loss: 0.2944 - loss: 0.3701 - regression_loss: 0.0757\n",
            "Epoch 14: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - class_loss: 0.2933 - loss: 0.3680 - regression_loss: 0.0747 - val_class_loss: 0.2530 - val_loss: 0.3120 - val_regression_loss: 0.0590 - learning_rate: 2.0000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - class_loss: 0.2980 - loss: 0.3677 - regression_loss: 0.0696\n",
            "Epoch 15: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - class_loss: 0.2966 - loss: 0.3662 - regression_loss: 0.0694 - val_class_loss: 0.2531 - val_loss: 0.3124 - val_regression_loss: 0.0593 - learning_rate: 2.0000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - class_loss: 0.2863 - loss: 0.3534 - regression_loss: 0.0671\n",
            "Epoch 16: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - class_loss: 0.2879 - loss: 0.3558 - regression_loss: 0.0674 - val_class_loss: 0.2533 - val_loss: 0.3117 - val_regression_loss: 0.0584 - learning_rate: 2.0000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - class_loss: 0.2970 - loss: 0.3696 - regression_loss: 0.0726\n",
            "Epoch 17: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - class_loss: 0.2956 - loss: 0.3684 - regression_loss: 0.0719 - val_class_loss: 0.2534 - val_loss: 0.3122 - val_regression_loss: 0.0587 - learning_rate: 2.0000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - class_loss: 0.2905 - loss: 0.3605 - regression_loss: 0.0700\n",
            "Epoch 18: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - class_loss: 0.2912 - loss: 0.3604 - regression_loss: 0.0696 - val_class_loss: 0.2536 - val_loss: 0.3128 - val_regression_loss: 0.0593 - learning_rate: 2.0000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - class_loss: 0.2834 - loss: 0.3515 - regression_loss: 0.0680\n",
            "Epoch 19: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - class_loss: 0.2832 - loss: 0.3514 - regression_loss: 0.0683 - val_class_loss: 0.2537 - val_loss: 0.3124 - val_regression_loss: 0.0587 - learning_rate: 2.0000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - class_loss: 0.2776 - loss: 0.3432 - regression_loss: 0.0656\n",
            "Epoch 20: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - class_loss: 0.2785 - loss: 0.3445 - regression_loss: 0.0660 - val_class_loss: 0.2539 - val_loss: 0.3124 - val_regression_loss: 0.0585 - learning_rate: 2.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - class_loss: 0.2785 - loss: 0.3439 - regression_loss: 0.0654\n",
            "Epoch 21: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - class_loss: 0.2791 - loss: 0.3456 - regression_loss: 0.0662 - val_class_loss: 0.2538 - val_loss: 0.3121 - val_regression_loss: 0.0583 - learning_rate: 2.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - class_loss: 0.2756 - loss: 0.3441 - regression_loss: 0.0685\n",
            "Epoch 22: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - class_loss: 0.2766 - loss: 0.3445 - regression_loss: 0.0683 - val_class_loss: 0.2536 - val_loss: 0.3116 - val_regression_loss: 0.0580 - learning_rate: 2.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - class_loss: 0.2956 - loss: 0.3643 - regression_loss: 0.0687\n",
            "Epoch 23: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - class_loss: 0.2939 - loss: 0.3624 - regression_loss: 0.0687 - val_class_loss: 0.2536 - val_loss: 0.3127 - val_regression_loss: 0.0590 - learning_rate: 2.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - class_loss: 0.2769 - loss: 0.3454 - regression_loss: 0.0678\n",
            "Epoch 24: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - class_loss: 0.2762 - loss: 0.3453 - regression_loss: 0.0677 - val_class_loss: 0.2538 - val_loss: 0.3123 - val_regression_loss: 0.0585 - learning_rate: 2.0000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - class_loss: 0.2739 - loss: 0.3407 - regression_loss: 0.0668\n",
            "Epoch 25: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - class_loss: 0.2743 - loss: 0.3419 - regression_loss: 0.0669 - val_class_loss: 0.2540 - val_loss: 0.3121 - val_regression_loss: 0.0581 - learning_rate: 2.0000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - class_loss: 0.2724 - loss: 0.3391 - regression_loss: 0.0664\n",
            "Epoch 26: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - class_loss: 0.2723 - loss: 0.3393 - regression_loss: 0.0664 - val_class_loss: 0.2541 - val_loss: 0.3124 - val_regression_loss: 0.0583 - learning_rate: 2.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - class_loss: 0.2809 - loss: 0.3444 - regression_loss: 0.0635\n",
            "Epoch 27: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - class_loss: 0.2800 - loss: 0.3453 - regression_loss: 0.0667 - val_class_loss: 0.2540 - val_loss: 0.3123 - val_regression_loss: 0.0583 - learning_rate: 2.0000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - class_loss: 0.2675 - loss: 0.3336 - regression_loss: 0.0661\n",
            "Epoch 28: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - class_loss: 0.2682 - loss: 0.3343 - regression_loss: 0.0664 - val_class_loss: 0.2540 - val_loss: 0.3125 - val_regression_loss: 0.0585 - learning_rate: 2.0000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - class_loss: 0.2785 - loss: 0.3469 - regression_loss: 0.0678\n",
            "Epoch 29: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - class_loss: 0.2778 - loss: 0.3466 - regression_loss: 0.0678 - val_class_loss: 0.2541 - val_loss: 0.3130 - val_regression_loss: 0.0589 - learning_rate: 2.0000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - class_loss: 0.2728 - loss: 0.3389 - regression_loss: 0.0656\n",
            "Epoch 30: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - class_loss: 0.2726 - loss: 0.3391 - regression_loss: 0.0657 - val_class_loss: 0.2542 - val_loss: 0.3122 - val_regression_loss: 0.0580 - learning_rate: 2.0000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - class_loss: 0.2734 - loss: 0.3404 - regression_loss: 0.0678\n",
            "Epoch 31: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - class_loss: 0.2741 - loss: 0.3403 - regression_loss: 0.0677 - val_class_loss: 0.2542 - val_loss: 0.3122 - val_regression_loss: 0.0580 - learning_rate: 2.0000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - class_loss: 0.2724 - loss: 0.3394 - regression_loss: 0.0671\n",
            "Epoch 32: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - class_loss: 0.2727 - loss: 0.3394 - regression_loss: 0.0671 - val_class_loss: 0.2542 - val_loss: 0.3122 - val_regression_loss: 0.0580 - learning_rate: 2.0000e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - class_loss: 0.2656 - loss: 0.3314 - regression_loss: 0.0656 \n",
            "Epoch 33: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - class_loss: 0.2655 - loss: 0.3314 - regression_loss: 0.0656 - val_class_loss: 0.2541 - val_loss: 0.3119 - val_regression_loss: 0.0578 - learning_rate: 2.0000e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - class_loss: 0.2665 - loss: 0.3318 - regression_loss: 0.0654\n",
            "Epoch 34: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - class_loss: 0.2661 - loss: 0.3327 - regression_loss: 0.0659 - val_class_loss: 0.2539 - val_loss: 0.3123 - val_regression_loss: 0.0583 - learning_rate: 2.0000e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - class_loss: 0.2657 - loss: 0.3338 - regression_loss: 0.0681\n",
            "Epoch 35: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - class_loss: 0.2672 - loss: 0.3338 - regression_loss: 0.0675 - val_class_loss: 0.2539 - val_loss: 0.3121 - val_regression_loss: 0.0582 - learning_rate: 2.0000e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - class_loss: 0.2736 - loss: 0.3410 - regression_loss: 0.0675\n",
            "Epoch 36: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - class_loss: 0.2723 - loss: 0.3402 - regression_loss: 0.0670 - val_class_loss: 0.2539 - val_loss: 0.3116 - val_regression_loss: 0.0577 - learning_rate: 2.0000e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - class_loss: 0.2642 - loss: 0.3290 - regression_loss: 0.0648\n",
            "Epoch 37: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - class_loss: 0.2638 - loss: 0.3298 - regression_loss: 0.0651 - val_class_loss: 0.2539 - val_loss: 0.3117 - val_regression_loss: 0.0578 - learning_rate: 2.0000e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - class_loss: 0.2695 - loss: 0.3334 - regression_loss: 0.0640\n",
            "Epoch 38: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - class_loss: 0.2690 - loss: 0.3339 - regression_loss: 0.0646 - val_class_loss: 0.2537 - val_loss: 0.3117 - val_regression_loss: 0.0580 - learning_rate: 2.0000e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - class_loss: 0.2604 - loss: 0.3286 - regression_loss: 0.0682\n",
            "Epoch 39: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - class_loss: 0.2608 - loss: 0.3283 - regression_loss: 0.0673 - val_class_loss: 0.2536 - val_loss: 0.3114 - val_regression_loss: 0.0579 - learning_rate: 2.0000e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - class_loss: 0.2645 - loss: 0.3284 - regression_loss: 0.0639\n",
            "Epoch 40: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - class_loss: 0.2646 - loss: 0.3289 - regression_loss: 0.0649 - val_class_loss: 0.2536 - val_loss: 0.3119 - val_regression_loss: 0.0583 - learning_rate: 2.0000e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - class_loss: 0.2653 - loss: 0.3299 - regression_loss: 0.0646\n",
            "Epoch 41: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - class_loss: 0.2643 - loss: 0.3302 - regression_loss: 0.0651 - val_class_loss: 0.2536 - val_loss: 0.3118 - val_regression_loss: 0.0581 - learning_rate: 2.0000e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - class_loss: 0.2677 - loss: 0.3344 - regression_loss: 0.0667\n",
            "Epoch 42: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - class_loss: 0.2677 - loss: 0.3340 - regression_loss: 0.0665 - val_class_loss: 0.2537 - val_loss: 0.3113 - val_regression_loss: 0.0577 - learning_rate: 2.0000e-04\n",
            "Epoch 43/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - class_loss: 0.2600 - loss: 0.3247 - regression_loss: 0.0647\n",
            "Epoch 43: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - class_loss: 0.2602 - loss: 0.3256 - regression_loss: 0.0654 - val_class_loss: 0.2537 - val_loss: 0.3114 - val_regression_loss: 0.0577 - learning_rate: 2.0000e-04\n",
            "Epoch 44/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - class_loss: 0.2570 - loss: 0.3225 - regression_loss: 0.0656\n",
            "Epoch 44: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - class_loss: 0.2577 - loss: 0.3234 - regression_loss: 0.0656 - val_class_loss: 0.2535 - val_loss: 0.3117 - val_regression_loss: 0.0582 - learning_rate: 2.0000e-04\n",
            "Epoch 45/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - class_loss: 0.2568 - loss: 0.3222 - regression_loss: 0.0652\n",
            "Epoch 45: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - class_loss: 0.2569 - loss: 0.3223 - regression_loss: 0.0652 - val_class_loss: 0.2534 - val_loss: 0.3112 - val_regression_loss: 0.0578 - learning_rate: 2.0000e-04\n",
            "Epoch 46/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - class_loss: 0.2598 - loss: 0.3252 - regression_loss: 0.0654\n",
            "Epoch 46: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - class_loss: 0.2597 - loss: 0.3256 - regression_loss: 0.0658 - val_class_loss: 0.2533 - val_loss: 0.3116 - val_regression_loss: 0.0583 - learning_rate: 2.0000e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - class_loss: 0.2562 - loss: 0.3211 - regression_loss: 0.0649\n",
            "Epoch 47: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - class_loss: 0.2565 - loss: 0.3213 - regression_loss: 0.0656 - val_class_loss: 0.2535 - val_loss: 0.3115 - val_regression_loss: 0.0581 - learning_rate: 2.0000e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - class_loss: 0.2518 - loss: 0.3168 - regression_loss: 0.0650\n",
            "Epoch 48: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - class_loss: 0.2531 - loss: 0.3177 - regression_loss: 0.0652 - val_class_loss: 0.2535 - val_loss: 0.3113 - val_regression_loss: 0.0578 - learning_rate: 2.0000e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - class_loss: 0.2583 - loss: 0.3223 - regression_loss: 0.0641\n",
            "Epoch 49: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - class_loss: 0.2577 - loss: 0.3221 - regression_loss: 0.0641 - val_class_loss: 0.2535 - val_loss: 0.3115 - val_regression_loss: 0.0580 - learning_rate: 2.0000e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - class_loss: 0.2589 - loss: 0.3249 - regression_loss: 0.0661\n",
            "Epoch 50: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - class_loss: 0.2594 - loss: 0.3253 - regression_loss: 0.0661 - val_class_loss: 0.2536 - val_loss: 0.3118 - val_regression_loss: 0.0582 - learning_rate: 2.0000e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - class_loss: 0.2619 - loss: 0.3279 - regression_loss: 0.0660\n",
            "Epoch 51: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - class_loss: 0.2616 - loss: 0.3272 - regression_loss: 0.0658 - val_class_loss: 0.2537 - val_loss: 0.3115 - val_regression_loss: 0.0577 - learning_rate: 2.0000e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - class_loss: 0.2538 - loss: 0.3186 - regression_loss: 0.0647\n",
            "Epoch 52: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - class_loss: 0.2540 - loss: 0.3190 - regression_loss: 0.0649 - val_class_loss: 0.2538 - val_loss: 0.3116 - val_regression_loss: 0.0578 - learning_rate: 2.0000e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - class_loss: 0.2589 - loss: 0.3227 - regression_loss: 0.0638\n",
            "Epoch 53: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - class_loss: 0.2589 - loss: 0.3229 - regression_loss: 0.0640 - val_class_loss: 0.2540 - val_loss: 0.3120 - val_regression_loss: 0.0580 - learning_rate: 2.0000e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - class_loss: 0.2550 - loss: 0.3206 - regression_loss: 0.0652\n",
            "Epoch 54: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - class_loss: 0.2548 - loss: 0.3206 - regression_loss: 0.0651 - val_class_loss: 0.2540 - val_loss: 0.3117 - val_regression_loss: 0.0577 - learning_rate: 2.0000e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - class_loss: 0.2578 - loss: 0.3228 - regression_loss: 0.0651\n",
            "Epoch 55: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - class_loss: 0.2579 - loss: 0.3227 - regression_loss: 0.0651 - val_class_loss: 0.2538 - val_loss: 0.3115 - val_regression_loss: 0.0576 - learning_rate: 2.0000e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - class_loss: 0.2503 - loss: 0.3181 - regression_loss: 0.0679\n",
            "Epoch 56: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - class_loss: 0.2513 - loss: 0.3187 - regression_loss: 0.0674 - val_class_loss: 0.2536 - val_loss: 0.3115 - val_regression_loss: 0.0579 - learning_rate: 2.0000e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - class_loss: 0.2563 - loss: 0.3226 - regression_loss: 0.0664\n",
            "Epoch 57: ReduceLROnPlateau reducing learning rate to 0.00017999999545281753.\n",
            "\n",
            "Epoch 57: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - class_loss: 0.2560 - loss: 0.3227 - regression_loss: 0.0661 - val_class_loss: 0.2532 - val_loss: 0.3110 - val_regression_loss: 0.0577 - learning_rate: 2.0000e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - class_loss: 0.2556 - loss: 0.3218 - regression_loss: 0.0662\n",
            "Epoch 58: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - class_loss: 0.2555 - loss: 0.3213 - regression_loss: 0.0662 - val_class_loss: 0.2531 - val_loss: 0.3109 - val_regression_loss: 0.0578 - learning_rate: 1.8000e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - class_loss: 0.2534 - loss: 0.3182 - regression_loss: 0.0648\n",
            "Epoch 59: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - class_loss: 0.2531 - loss: 0.3183 - regression_loss: 0.0654 - val_class_loss: 0.2530 - val_loss: 0.3108 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - class_loss: 0.2596 - loss: 0.3229 - regression_loss: 0.0633\n",
            "Epoch 60: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - class_loss: 0.2590 - loss: 0.3227 - regression_loss: 0.0635 - val_class_loss: 0.2530 - val_loss: 0.3106 - val_regression_loss: 0.0576 - learning_rate: 1.8000e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - class_loss: 0.2536 - loss: 0.3179 - regression_loss: 0.0643\n",
            "Epoch 61: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - class_loss: 0.2538 - loss: 0.3186 - regression_loss: 0.0647 - val_class_loss: 0.2530 - val_loss: 0.3106 - val_regression_loss: 0.0576 - learning_rate: 1.8000e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - class_loss: 0.2562 - loss: 0.3208 - regression_loss: 0.0646\n",
            "Epoch 62: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - class_loss: 0.2558 - loss: 0.3209 - regression_loss: 0.0647 - val_class_loss: 0.2529 - val_loss: 0.3113 - val_regression_loss: 0.0584 - learning_rate: 1.8000e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - class_loss: 0.2577 - loss: 0.3239 - regression_loss: 0.0661\n",
            "Epoch 63: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - class_loss: 0.2576 - loss: 0.3233 - regression_loss: 0.0657 - val_class_loss: 0.2529 - val_loss: 0.3106 - val_regression_loss: 0.0576 - learning_rate: 1.8000e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - class_loss: 0.2576 - loss: 0.3224 - regression_loss: 0.0648\n",
            "Epoch 64: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - class_loss: 0.2576 - loss: 0.3222 - regression_loss: 0.0647 - val_class_loss: 0.2528 - val_loss: 0.3102 - val_regression_loss: 0.0574 - learning_rate: 1.8000e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - class_loss: 0.2531 - loss: 0.3191 - regression_loss: 0.0660\n",
            "Epoch 65: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - class_loss: 0.2536 - loss: 0.3196 - regression_loss: 0.0660 - val_class_loss: 0.2526 - val_loss: 0.3103 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 66/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - class_loss: 0.2551 - loss: 0.3208 - regression_loss: 0.0657\n",
            "Epoch 66: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - class_loss: 0.2544 - loss: 0.3199 - regression_loss: 0.0654 - val_class_loss: 0.2525 - val_loss: 0.3102 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 67/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - class_loss: 0.2524 - loss: 0.3187 - regression_loss: 0.0663\n",
            "Epoch 67: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - class_loss: 0.2525 - loss: 0.3181 - regression_loss: 0.0663 - val_class_loss: 0.2525 - val_loss: 0.3107 - val_regression_loss: 0.0582 - learning_rate: 1.8000e-04\n",
            "Epoch 68/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - class_loss: 0.2496 - loss: 0.3150 - regression_loss: 0.0655\n",
            "Epoch 68: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - class_loss: 0.2498 - loss: 0.3151 - regression_loss: 0.0654 - val_class_loss: 0.2525 - val_loss: 0.3102 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 69/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - class_loss: 0.2551 - loss: 0.3240 - regression_loss: 0.0689\n",
            "Epoch 69: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - class_loss: 0.2544 - loss: 0.3226 - regression_loss: 0.0681 - val_class_loss: 0.2525 - val_loss: 0.3100 - val_regression_loss: 0.0575 - learning_rate: 1.8000e-04\n",
            "Epoch 70/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - class_loss: 0.2489 - loss: 0.3134 - regression_loss: 0.0646\n",
            "Epoch 70: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - class_loss: 0.2498 - loss: 0.3144 - regression_loss: 0.0648 - val_class_loss: 0.2524 - val_loss: 0.3100 - val_regression_loss: 0.0576 - learning_rate: 1.8000e-04\n",
            "Epoch 71/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - class_loss: 0.2560 - loss: 0.3221 - regression_loss: 0.0660\n",
            "Epoch 71: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - class_loss: 0.2560 - loss: 0.3221 - regression_loss: 0.0658 - val_class_loss: 0.2522 - val_loss: 0.3099 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 72/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - class_loss: 0.2503 - loss: 0.3147 - regression_loss: 0.0644\n",
            "Epoch 72: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - class_loss: 0.2503 - loss: 0.3150 - regression_loss: 0.0646 - val_class_loss: 0.2522 - val_loss: 0.3098 - val_regression_loss: 0.0576 - learning_rate: 1.8000e-04\n",
            "Epoch 73/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - class_loss: 0.2521 - loss: 0.3178 - regression_loss: 0.0653\n",
            "Epoch 73: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - class_loss: 0.2520 - loss: 0.3179 - regression_loss: 0.0651 - val_class_loss: 0.2521 - val_loss: 0.3096 - val_regression_loss: 0.0575 - learning_rate: 1.8000e-04\n",
            "Epoch 74/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - class_loss: 0.2548 - loss: 0.3237 - regression_loss: 0.0686\n",
            "Epoch 74: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - class_loss: 0.2543 - loss: 0.3230 - regression_loss: 0.0682 - val_class_loss: 0.2521 - val_loss: 0.3096 - val_regression_loss: 0.0575 - learning_rate: 1.8000e-04\n",
            "Epoch 75/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - class_loss: 0.2520 - loss: 0.3176 - regression_loss: 0.0658\n",
            "Epoch 75: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - class_loss: 0.2521 - loss: 0.3175 - regression_loss: 0.0658 - val_class_loss: 0.2523 - val_loss: 0.3099 - val_regression_loss: 0.0576 - learning_rate: 1.8000e-04\n",
            "Epoch 76/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - class_loss: 0.2480 - loss: 0.3111 - regression_loss: 0.0634\n",
            "Epoch 76: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - class_loss: 0.2484 - loss: 0.3115 - regression_loss: 0.0637 - val_class_loss: 0.2524 - val_loss: 0.3101 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 77/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - class_loss: 0.2507 - loss: 0.3160 - regression_loss: 0.0654\n",
            "Epoch 77: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - class_loss: 0.2509 - loss: 0.3161 - regression_loss: 0.0654 - val_class_loss: 0.2525 - val_loss: 0.3101 - val_regression_loss: 0.0576 - learning_rate: 1.8000e-04\n",
            "Epoch 78/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - class_loss: 0.2513 - loss: 0.3168 - regression_loss: 0.0656\n",
            "Epoch 78: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - class_loss: 0.2506 - loss: 0.3166 - regression_loss: 0.0654 - val_class_loss: 0.2525 - val_loss: 0.3100 - val_regression_loss: 0.0575 - learning_rate: 1.8000e-04\n",
            "Epoch 79/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - class_loss: 0.2510 - loss: 0.3162 - regression_loss: 0.0652\n",
            "Epoch 79: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - class_loss: 0.2514 - loss: 0.3163 - regression_loss: 0.0655 - val_class_loss: 0.2525 - val_loss: 0.3101 - val_regression_loss: 0.0575 - learning_rate: 1.8000e-04\n",
            "Epoch 80/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - class_loss: 0.2521 - loss: 0.3166 - regression_loss: 0.0642\n",
            "Epoch 80: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - class_loss: 0.2519 - loss: 0.3166 - regression_loss: 0.0642 - val_class_loss: 0.2525 - val_loss: 0.3102 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 81/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - class_loss: 0.2505 - loss: 0.3164 - regression_loss: 0.0664\n",
            "Epoch 81: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - class_loss: 0.2508 - loss: 0.3165 - regression_loss: 0.0665 - val_class_loss: 0.2526 - val_loss: 0.3104 - val_regression_loss: 0.0578 - learning_rate: 1.8000e-04\n",
            "Epoch 82/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - class_loss: 0.2529 - loss: 0.3194 - regression_loss: 0.0664\n",
            "Epoch 82: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - class_loss: 0.2531 - loss: 0.3192 - regression_loss: 0.0660 - val_class_loss: 0.2526 - val_loss: 0.3105 - val_regression_loss: 0.0578 - learning_rate: 1.8000e-04\n",
            "Epoch 83/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - class_loss: 0.2490 - loss: 0.3141 - regression_loss: 0.0651\n",
            "Epoch 83: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - class_loss: 0.2489 - loss: 0.3139 - regression_loss: 0.0651 - val_class_loss: 0.2528 - val_loss: 0.3103 - val_regression_loss: 0.0575 - learning_rate: 1.8000e-04\n",
            "Epoch 84/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - class_loss: 0.2528 - loss: 0.3154 - regression_loss: 0.0626\n",
            "Epoch 84: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - class_loss: 0.2519 - loss: 0.3155 - regression_loss: 0.0632 - val_class_loss: 0.2529 - val_loss: 0.3106 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 85/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - class_loss: 0.2474 - loss: 0.3141 - regression_loss: 0.0666\n",
            "Epoch 85: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - class_loss: 0.2471 - loss: 0.3139 - regression_loss: 0.0660 - val_class_loss: 0.2531 - val_loss: 0.3107 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 86/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - class_loss: 0.2463 - loss: 0.3106 - regression_loss: 0.0642\n",
            "Epoch 86: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - class_loss: 0.2476 - loss: 0.3118 - regression_loss: 0.0646 - val_class_loss: 0.2532 - val_loss: 0.3108 - val_regression_loss: 0.0576 - learning_rate: 1.8000e-04\n",
            "Epoch 87/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - class_loss: 0.2513 - loss: 0.3180 - regression_loss: 0.0667\n",
            "Epoch 87: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - class_loss: 0.2510 - loss: 0.3173 - regression_loss: 0.0664 - val_class_loss: 0.2533 - val_loss: 0.3111 - val_regression_loss: 0.0578 - learning_rate: 1.8000e-04\n",
            "Epoch 88/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - class_loss: 0.2487 - loss: 0.3147 - regression_loss: 0.0658\n",
            "Epoch 88: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - class_loss: 0.2486 - loss: 0.3145 - regression_loss: 0.0656 - val_class_loss: 0.2534 - val_loss: 0.3111 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 89/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - class_loss: 0.2501 - loss: 0.3147 - regression_loss: 0.0646\n",
            "Epoch 89: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - class_loss: 0.2503 - loss: 0.3150 - regression_loss: 0.0645 - val_class_loss: 0.2534 - val_loss: 0.3110 - val_regression_loss: 0.0576 - learning_rate: 1.8000e-04\n",
            "Epoch 90/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - class_loss: 0.2524 - loss: 0.3165 - regression_loss: 0.0640\n",
            "Epoch 90: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - class_loss: 0.2523 - loss: 0.3169 - regression_loss: 0.0641 - val_class_loss: 0.2533 - val_loss: 0.3110 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 91/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - class_loss: 0.2477 - loss: 0.3126 - regression_loss: 0.0649\n",
            "Epoch 91: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - class_loss: 0.2484 - loss: 0.3130 - regression_loss: 0.0649 - val_class_loss: 0.2532 - val_loss: 0.3108 - val_regression_loss: 0.0576 - learning_rate: 1.8000e-04\n",
            "Epoch 92/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - class_loss: 0.2526 - loss: 0.3158 - regression_loss: 0.0632\n",
            "Epoch 92: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - class_loss: 0.2524 - loss: 0.3165 - regression_loss: 0.0649 - val_class_loss: 0.2532 - val_loss: 0.3116 - val_regression_loss: 0.0584 - learning_rate: 1.8000e-04\n",
            "Epoch 93/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - class_loss: 0.2515 - loss: 0.3154 - regression_loss: 0.0639\n",
            "Epoch 93: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - class_loss: 0.2510 - loss: 0.3156 - regression_loss: 0.0642 - val_class_loss: 0.2532 - val_loss: 0.3116 - val_regression_loss: 0.0584 - learning_rate: 1.8000e-04\n",
            "Epoch 94/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - class_loss: 0.2479 - loss: 0.3141 - regression_loss: 0.0662\n",
            "Epoch 94: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - class_loss: 0.2481 - loss: 0.3140 - regression_loss: 0.0658 - val_class_loss: 0.2532 - val_loss: 0.3107 - val_regression_loss: 0.0575 - learning_rate: 1.8000e-04\n",
            "Epoch 95/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - class_loss: 0.2537 - loss: 0.3210 - regression_loss: 0.0675 \n",
            "Epoch 95: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step - class_loss: 0.2539 - loss: 0.3207 - regression_loss: 0.0672 - val_class_loss: 0.2531 - val_loss: 0.3105 - val_regression_loss: 0.0574 - learning_rate: 1.8000e-04\n",
            "Epoch 96/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - class_loss: 0.2521 - loss: 0.3204 - regression_loss: 0.0682\n",
            "Epoch 96: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 110ms/step - class_loss: 0.2517 - loss: 0.3199 - regression_loss: 0.0678 - val_class_loss: 0.2530 - val_loss: 0.3107 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 97/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - class_loss: 0.2500 - loss: 0.3155 - regression_loss: 0.0655\n",
            "Epoch 97: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 104ms/step - class_loss: 0.2500 - loss: 0.3155 - regression_loss: 0.0654 - val_class_loss: 0.2530 - val_loss: 0.3106 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 98/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - class_loss: 0.2512 - loss: 0.3153 - regression_loss: 0.0640\n",
            "Epoch 98: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - class_loss: 0.2511 - loss: 0.3154 - regression_loss: 0.0640 - val_class_loss: 0.2530 - val_loss: 0.3105 - val_regression_loss: 0.0576 - learning_rate: 1.8000e-04\n",
            "Epoch 99/100\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - class_loss: 0.2490 - loss: 0.3116 - regression_loss: 0.0629 \n",
            "Epoch 99: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - class_loss: 0.2492 - loss: 0.3120 - regression_loss: 0.0632 - val_class_loss: 0.2530 - val_loss: 0.3108 - val_regression_loss: 0.0577 - learning_rate: 1.8000e-04\n",
            "Epoch 100/100\n",
            "\u001b[1m6/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - class_loss: 0.2512 - loss: 0.3141 - regression_loss: 0.0629\n",
            "Epoch 100: val_loss did not improve from 0.30801\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - class_loss: 0.2517 - loss: 0.3146 - regression_loss: 0.0643 - val_class_loss: 0.2529 - val_loss: 0.3109 - val_regression_loss: 0.0580 - learning_rate: 1.8000e-04\n"
          ]
        }
      ],
      "source": [
        "history = final_model.fit(\n",
        "    [X_train, X_train_text],\n",
        "    [Y_train, Y_train2],\n",
        "    epochs=100,\n",
        "    batch_size=256,\n",
        "    verbose=1,\n",
        "    validation_data=([X_test, X_test_text], [Y_test, Y_test2]),\n",
        "    callbacks=[reduce_lr, checkpointer],\n",
        "    shuffle=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "QMgcmqtouqWD",
        "outputId": "819124eb-0b8e-4707-9a45-dcfcec6fc634"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "\n",
        "plt.plot(history.history['regression_loss'], label='train_regression_loss')\n",
        "plt.plot(history.history['val_regression_loss'], label='val_regression_loss')\n",
        "plt.plot(history.history['class_loss'], label='train_class_loss')\n",
        "plt.plot(history.history['val_class_loss'], label='val_class_loss')\n",
        "\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPk4KNS9he1n9v8WRPh+bkw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}